{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Training and Evaluation\n",
    "\n",
    "Train the final model using the best hyperparameters from Optuna optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 18:49:16.152366: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-15 18:49:16.189943: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-15 18:49:16.190775: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-15 18:49:16.856003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Setup paths\n",
    "notebook_dir = Path(os.path.abspath('')).parent\n",
    "build_dir = notebook_dir.parent\n",
    "sys.path.append(str(notebook_dir))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.modeling.architecture import ModelBuilder\n",
    "from src.modeling.trainer import ModelTrainer\n",
    "from src.preprocessing.preprocessor import FeaturePreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "n_layers: 4\n",
      "activation: leaky_relu\n",
      "dropout_rate: 0.1693670900721545\n",
      "apply_batch_norm: True\n",
      "optimizer: nadam\n",
      "learning_rate: 0.0005535645533075388\n",
      "batch_size: 1560\n",
      "regularization: None\n",
      "n_units_0: 887\n",
      "n_units_1: 660\n",
      "n_units_2: 748\n",
      "n_units_3: 697\n"
     ]
    }
   ],
   "source": [
    "# Load best parameters from Optuna\n",
    "with open(os.path.join(build_dir, 'optuna_results/best_parameters.json'), 'r') as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "feature_cols = ['mH2', 'mHD', 'mAD', 'mHDp', 'alpha', 'L2', 'L8', 'vs', 'm22sq']\n",
    "label_cols = ['valid_BFB', 'valid_Uni', 'valid_STU', 'valid_Higgs']\n",
    "\n",
    "# Load preprocessor\n",
    "preprocessor = FeaturePreprocessor.load_transformers(\n",
    "    os.path.join(build_dir, 'preprocessor')\n",
    ")\n",
    "\n",
    "# Load all datasets\n",
    "train_data = pd.read_csv(os.path.join(build_dir, 'data_splits/train_set.tsv'), sep='\\t')\n",
    "val_data = pd.read_csv(os.path.join(build_dir, 'data_splits/val_set.tsv'), sep='\\t')\n",
    "test_data = pd.read_csv(os.path.join(build_dir, 'data_splits/test_set.tsv'), sep='\\t')\n",
    "\n",
    "# Preprocess all sets\n",
    "X_train = preprocessor.transform(train_data[feature_cols])\n",
    "y_train = train_data[label_cols]\n",
    "\n",
    "X_val = preprocessor.transform(val_data[feature_cols])\n",
    "y_val = val_data[label_cols]\n",
    "\n",
    "X_test = preprocessor.transform(test_data[feature_cols])\n",
    "y_test = test_data[label_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model builder\n",
    "builder = ModelBuilder(\n",
    "    input_shape=(len(feature_cols),),\n",
    "    num_outputs=len(label_cols)\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = ModelTrainer(\n",
    "    model_builder=builder,\n",
    "    feature_cols=feature_cols,\n",
    "    label_cols=label_cols,\n",
    "    output_dir=os.path.join(build_dir, 'final_model')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 18:49:27.188043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: maienPC\n",
      "2025-01-15 18:49:27.188069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: maienPC\n",
      "2025-01-15 18:49:27.188232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2025-01-15 18:49:27.188268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 470.239.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53/53 [==============================] - 12s 181ms/step - loss: 0.1585 - subset_accuracy: 0.4089 - hamming_loss: 0.2028 - matthews_correlation: 0.5884 - macro_f1_score: 0.8232 - val_loss: 0.1371 - val_subset_accuracy: 0.3605 - val_hamming_loss: 0.2393 - val_matthews_correlation: 0.5605 - val_macro_f1_score: 0.8305\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 9s 175ms/step - loss: 0.0939 - subset_accuracy: 0.5359 - hamming_loss: 0.1423 - matthews_correlation: 0.7108 - macro_f1_score: 0.8785 - val_loss: 0.1553 - val_subset_accuracy: 0.3123 - val_hamming_loss: 0.2809 - val_matthews_correlation: 0.4778 - val_macro_f1_score: 0.7969\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 10s 183ms/step - loss: 0.0783 - subset_accuracy: 0.5927 - hamming_loss: 0.1207 - matthews_correlation: 0.7549 - macro_f1_score: 0.8970 - val_loss: 0.1839 - val_subset_accuracy: 0.3050 - val_hamming_loss: 0.2823 - val_matthews_correlation: 0.4714 - val_macro_f1_score: 0.7939\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 9s 171ms/step - loss: 0.0693 - subset_accuracy: 0.6312 - hamming_loss: 0.1069 - matthews_correlation: 0.7831 - macro_f1_score: 0.9086 - val_loss: 0.1872 - val_subset_accuracy: 0.3496 - val_hamming_loss: 0.2499 - val_matthews_correlation: 0.5247 - val_macro_f1_score: 0.8115\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 10s 196ms/step - loss: 0.0632 - subset_accuracy: 0.6595 - hamming_loss: 0.0976 - matthews_correlation: 0.8021 - macro_f1_score: 0.9166 - val_loss: 0.1636 - val_subset_accuracy: 0.4072 - val_hamming_loss: 0.2142 - val_matthews_correlation: 0.5856 - val_macro_f1_score: 0.8328\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 8s 153ms/step - loss: 0.0586 - subset_accuracy: 0.6819 - hamming_loss: 0.0902 - matthews_correlation: 0.8173 - macro_f1_score: 0.9229 - val_loss: 0.1343 - val_subset_accuracy: 0.4951 - val_hamming_loss: 0.1667 - val_matthews_correlation: 0.6816 - val_macro_f1_score: 0.8695\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 8s 154ms/step - loss: 0.0554 - subset_accuracy: 0.6964 - hamming_loss: 0.0848 - matthews_correlation: 0.8282 - macro_f1_score: 0.9277 - val_loss: 0.1005 - val_subset_accuracy: 0.5778 - val_hamming_loss: 0.1309 - val_matthews_correlation: 0.7484 - val_macro_f1_score: 0.8955\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 8s 156ms/step - loss: 0.0524 - subset_accuracy: 0.7155 - hamming_loss: 0.0796 - matthews_correlation: 0.8389 - macro_f1_score: 0.9320 - val_loss: 0.0735 - val_subset_accuracy: 0.6545 - val_hamming_loss: 0.1011 - val_matthews_correlation: 0.8051 - val_macro_f1_score: 0.9184\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 8s 150ms/step - loss: 0.0497 - subset_accuracy: 0.7301 - hamming_loss: 0.0745 - matthews_correlation: 0.8494 - macro_f1_score: 0.9365 - val_loss: 0.0543 - val_subset_accuracy: 0.7246 - val_hamming_loss: 0.0776 - val_matthews_correlation: 0.8487 - val_macro_f1_score: 0.9358\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 9s 173ms/step - loss: 0.0471 - subset_accuracy: 0.7440 - hamming_loss: 0.0706 - matthews_correlation: 0.8573 - macro_f1_score: 0.9399 - val_loss: 0.0450 - val_subset_accuracy: 0.7671 - val_hamming_loss: 0.0642 - val_matthews_correlation: 0.8724 - val_macro_f1_score: 0.9457\n"
     ]
    }
   ],
   "source": [
    "# Train final model\n",
    "model = trainer.train(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    params=best_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545/545 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "trainer.evaluate(\n",
    "    model=model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
